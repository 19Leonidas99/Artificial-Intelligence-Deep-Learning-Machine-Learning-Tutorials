{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Tensor Comprehensions\n",
    "\n",
    "```shell\n",
    "$ conda install -y -c pytorch -c tensorcomp tensor_comprehensions\n",
    "```\n",
    "Note: Won;t work on your mac, this is my Ubuntu server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensor_comprehensions as tc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang = \"\"\"\n",
    "def matmul(float(M,N) A, float(N,K) B) -> (output) {\n",
    "  output(i, j) +=! A(i, kk) * B(kk, j)\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: No mapping options passed, 'naive' type mapping options will be used and will likely have bad performance. See help(your_layer.__call__) for setting mapping options.\n"
     ]
    }
   ],
   "source": [
    "matmul = tc.define(lang, name=\"matmul\")\n",
    "mat1, mat2 = torch.randn(3, 4).cuda(), torch.randn(4, 5).cuda()\n",
    "out = matmul(mat1, mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.7536  2.7156 -2.3936 -0.1357 -0.5720\n",
       "-1.7986  2.9340 -0.0688  0.3831  1.3774\n",
       "-0.6450 -0.8190  0.2687  1.2483 -1.0357\n",
       "[torch.cuda.FloatTensor of size 3x5 (GPU 0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch layers in Tensor Comprehensions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of mapping option\n",
    "\n",
    "Default Mapping: We provide various default options that can be chosen to closely represent the kernel. The defaults provided are:\n",
    "\n",
    "- `pointwise, color=red`: if kernel resembles a pointwise operation\n",
    "- `mlp`: if kernel resembles an Linear layer operation\n",
    "- `conv`: if kernel resembles a convolution operation\n",
    "- `group_conv`: if kernel resembles a group convolution operation\n",
    "- `naive`: if none of the above, then chose naive default <-- This is why we get the warning\n",
    "<font color='red'>bar</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specifying mapping options\n",
    "matmul = tc.define(lang, name=\"matmul\")\n",
    "mat1, mat2 = torch.randn(100, 400).cuda(), torch.randn(400, 500).cuda()\n",
    "out2 = matmul(mat1, mat2, options=tc.Options(\"mlp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-2.8489e+01  2.8404e+01  2.7415e+01  ...  -1.2269e+00 -3.6730e+01 -9.9155e+00\n",
       "-1.7115e+01 -1.1121e+01 -3.0050e+01  ...  -6.7856e-01  3.8747e+00 -4.0622e+01\n",
       "-2.5548e+01 -1.4382e+01 -8.0181e+00  ...   1.3411e+01  3.2021e+00  8.1902e-01\n",
       "                ...                   ⋱                   ...                \n",
       " 2.2978e+01 -2.4670e+01 -2.3970e+01  ...   1.5433e+00 -2.5969e+01 -2.9356e+00\n",
       "-5.9101e+00 -1.6884e+01  2.4006e+01  ...  -5.9949e+00 -2.3709e+01  1.0794e+01\n",
       " 1.4939e+01 -1.3193e+01 -5.4434e+01  ...   1.8283e+01 -3.1528e+01 -1.4516e+00\n",
       "[torch.cuda.FloatTensor of size 100x500 (GPU 0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: No mapping options passed, 'naive' type mapping options will be used and will likely have bad performance. See help(your_layer.__call__) for setting mapping options.\n",
      "[WARNING]: No mapping options passed, 'naive' type mapping options will be used and will likely have bad performance. See help(your_layer.__call__) for setting mapping options.\n",
      "Variable containing:\n",
      " 0.7346  0.0796  0.7190  0.4359  0.0805\n",
      " 0.9850 -0.2511  0.8243  0.4254 -0.5436\n",
      "-0.6071 -1.8995 -2.1216 -1.7089  2.4164\n",
      "[torch.cuda.FloatTensor of size 3x5 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "  7.0490  14.0783 -31.0707  ...   -4.0171 -33.0912 -11.7724\n",
      " -2.8942  25.8813 -32.5398  ...   13.7943  22.6707 -10.7807\n",
      " 28.8399  14.7229 -10.8210  ...  -25.2078  -4.4914  -9.6010\n",
      "           ...               ⋱              ...            \n",
      "-21.9716   3.9036  19.7386  ...   -8.0826   8.1482  -4.7208\n",
      " 23.0311  31.1360   5.9420  ...  -27.8790 -14.3433  -4.7663\n",
      "-34.4321  20.4107 -10.2030  ...   -3.9045  17.4901  29.1404\n",
      "[torch.cuda.FloatTensor of size 100x500 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using reduction operators\n",
    "# providing different input sizes for the same comprehension\n",
    "\n",
    "matmul = tc.define(lang, name=\"matmul\")\n",
    "mat1, mat2 = torch.randn(3, 4).cuda(), torch.randn(4, 5).cuda()\n",
    "out = matmul(mat1, mat2)\n",
    "\n",
    "# different input sizes\n",
    "mat3, mat4 = torch.randn(100, 400).cuda(), torch.randn(400, 500).cuda()\n",
    "out2 = matmul(mat3, mat4)\n",
    "print(out)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple TC definitions\n",
    "\n",
    "Let’s say you want to define all of your TCs in one string and later use that string for running different operations defined in the string. You an do so easily. You can define a <font color='blue'>lang</font> variable that holds the TC definition for all your operations. Every time you want to run a different operation, you can make a <font color='blue'>tc.define</font> call on the <font color='blue'>lang</font> variable, specify the <font color='blue'>name</font> corresponding to the operation definition and get the TC layer for it. Below is an example for how to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: No mapping options passed, 'naive' type mapping options will be used and will likely have bad performance. See help(your_layer.__call__) for setting mapping options.\n",
      "[WARNING]: No mapping options passed, 'naive' type mapping options will be used and will likely have bad performance. See help(your_layer.__call__) for setting mapping options.\n"
     ]
    }
   ],
   "source": [
    "lang = \"\"\"\n",
    "def matmul(float(M,N) A, float(N,K) B) -> (output) {\n",
    "  output(i, j) +=! A(i, kk) * B(kk, j)\n",
    "}\n",
    "def abs(float(M, N) A) -> (O1) {\n",
    "  O1(m, n) = fabs(A(m, n))\n",
    "}\n",
    "\"\"\"\n",
    "matmul = tc.define(lang, name=\"matmul\")\n",
    "mat1, mat2 = torch.randn(3, 4).cuda(), torch.randn(4, 5).cuda()\n",
    "out = matmul(mat1, mat2)\n",
    "\n",
    "abs = tc.define(lang, name=\"abs\")\n",
    "A = torch.randn(3, 4).cuda()\n",
    "out = abs(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.6016  0.0163  1.9639  0.7835\n",
       " 0.3498  0.3417  0.8138  0.2829\n",
       " 0.4028  0.3834  0.9464  0.6010\n",
       "[torch.cuda.FloatTensor of size 3x4 (GPU 0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Writing layers with Scalars\n",
    "\n",
    "- **Option 1**: Pass a constants dictionary to the tc.define call as demo'ed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"\"\"\n",
    "def avgpool(float(B, C, H, W) input) -> (output) {{\n",
    "    output(b, c, h, w) += input(b, c, h * {sH} + kh, w * {sW} + kw) where kh in 0:{kH}, kw in 0:{kW}\n",
    "}}\n",
    "\"\"\"\n",
    "avgpool = tc.define(lang, name=\"avgpool\", constants={\"sH\":1, \"sW\":1, \"kH\":2, \"kW\":2})\n",
    "inp = torch.ones(32, 3, 10, 10).cuda()\n",
    "out4 = avgpool(inp, options=tc.Options(\"mlp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,0 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(0 ,1 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(0 ,2 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ⋮ \n",
       "\n",
       "(1 ,0 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(1 ,1 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(1 ,2 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ⋮ \n",
       "\n",
       "(2 ,0 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(2 ,1 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(2 ,2 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "...   \n",
       "     ⋮ \n",
       "\n",
       "(29,0 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(29,1 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(29,2 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ⋮ \n",
       "\n",
       "(30,0 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(30,1 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(30,2 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ⋮ \n",
       "\n",
       "(31,0 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(31,1 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(31,2 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "[torch.cuda.FloatTensor of size 32x3x9x9 (GPU 0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 : Format the string using python regex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: No mapping options passed, 'naive' type mapping options will be used and will likely have bad performance. See help(your_layer.__call__) for setting mapping options.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "LANG=\"\"\"\n",
    "def avgpool(float(B, C, H, W) input) -> (output) {\n",
    "    output(b, c, h, w) += input(b, c, h * <sh> + kh, w * <sw> + kw) where kh in 0:<kH>, kw in 0:<kW>\n",
    "}\n",
    "\"\"\"\n",
    "sH, sW, kH, kW = 1, 1, 2, 2\n",
    "LANG = re.sub('<sh>', str(sH), LANG)\n",
    "LANG = re.sub('<sw>', str(sW), LANG)\n",
    "LANG = re.sub('<kH>', str(kH), LANG)\n",
    "LANG = re.sub('<kW>', str(kW), LANG)\n",
    "avgpool = tc.define(LANG, name=\"avgpool\")\n",
    "inp = torch.ones(1, 1, 4, 4).cuda()\n",
    "out5 = avgpool(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually injecting external CUDA code\n",
    "\n",
    "If you have an external efficient CUDA code that you want to use rather than the CUDA code that TC generates, you can inject your code easily. For this, you need to create a string which has the CUDA code you want to inject and you need to pass the name of the kernel and the CUDA code string to the `tc.define` call.\n",
    "\n",
    "As an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: No mapping options passed, 'naive' type mapping options will be used and will likely have bad performance. See help(your_layer.__call__) for setting mapping options.\n"
     ]
    }
   ],
   "source": [
    "lang = \"\"\"\n",
    "def add(float(N) A, float(N) B) -> (output) {\n",
    "    output(i) = A(i) + B(i) + 1\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "cuda_code = \"\"\"\n",
    "extern \"C\"{\n",
    "__global__ void my_add(float* __restrict__ output, const float* __restrict__ A, const float* __restrict B)\n",
    "{\n",
    "    int t = threadIdx.x;\n",
    "    output[t] = A[t] + B[t];\n",
    "}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "add = tc.define(lang, name=\"add\", inject_kernel=\"my_add\", cuda_code=cuda_code)\n",
    "a, b = torch.randn(100).cuda(), torch.randn(100).cuda()\n",
    "out6 = add(a, b, grid=[1, 1, 1], block=[100, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 3.6025\n",
       " 1.2179\n",
       "-0.7659\n",
       "-1.0284\n",
       "-0.8866\n",
       " 0.8470\n",
       " 1.1899\n",
       " 0.5136\n",
       "-0.5460\n",
       "-1.5441\n",
       "-0.3539\n",
       "-2.4137\n",
       "-0.4631\n",
       "-0.0619\n",
       "-1.6390\n",
       "-2.0870\n",
       " 0.8110\n",
       "-2.0814\n",
       "-0.0284\n",
       "-2.3822\n",
       " 1.4707\n",
       " 3.8718\n",
       "-0.7765\n",
       "-3.1565\n",
       "-1.9431\n",
       " 0.5884\n",
       "-2.8776\n",
       "-1.8268\n",
       "-0.1874\n",
       "-0.9072\n",
       "-0.1957\n",
       "-0.3769\n",
       " 1.9516\n",
       " 1.2142\n",
       " 0.8183\n",
       "-0.5128\n",
       " 1.2283\n",
       "-0.1149\n",
       "-1.0397\n",
       " 1.7810\n",
       "-1.0703\n",
       "-0.1569\n",
       " 1.1433\n",
       " 0.9462\n",
       "-0.9802\n",
       "-3.1964\n",
       "-3.1568\n",
       " 0.0119\n",
       "-0.2752\n",
       "-2.6597\n",
       "-0.4386\n",
       "-0.4265\n",
       " 0.3517\n",
       "-2.6374\n",
       " 1.8531\n",
       " 0.1807\n",
       " 0.7856\n",
       "-1.4368\n",
       " 1.1380\n",
       "-0.2945\n",
       "-1.3483\n",
       "-1.9964\n",
       "-0.3021\n",
       " 1.3201\n",
       "-1.2708\n",
       "-0.0591\n",
       " 0.7631\n",
       "-2.1894\n",
       "-0.3149\n",
       " 2.1207\n",
       " 0.3688\n",
       " 3.3965\n",
       " 0.9015\n",
       "-0.3889\n",
       "-1.1659\n",
       "-2.7698\n",
       " 0.3136\n",
       " 0.0787\n",
       "-0.4413\n",
       "-1.5912\n",
       " 1.1120\n",
       "-1.2563\n",
       "-0.1105\n",
       " 1.5039\n",
       " 1.1011\n",
       " 2.0790\n",
       " 0.3309\n",
       " 2.8792\n",
       "-1.4424\n",
       "-3.8384\n",
       " 0.0753\n",
       "-1.0948\n",
       " 0.7200\n",
       "-1.3414\n",
       " 1.2493\n",
       "-3.1341\n",
       " 2.5363\n",
       "-0.9303\n",
       "-0.0479\n",
       " 0.7473\n",
       "[torch.cuda.FloatTensor of size 100 (GPU 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
