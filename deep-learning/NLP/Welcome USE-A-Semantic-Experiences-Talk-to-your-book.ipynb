{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Mighty USE of NLP -- Universal Sentence Encoder\n",
    "\n",
    "This notebook illustrates how to access the Universal Sentence Encoder and use it for sentence similarity and sentence classification tasks.\n",
    "\n",
    "The Universal Sentence Encoder makes getting sentence level embeddings as easy as it has historically been to lookup the embeddings for individual words. The sentence embeddings can then be trivially used to compute sentence level meaning similarity as well as to enable better performance on downstream classification tasks using less supervised training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 2\n",
    "\n",
    "- Get Universal Sentence Encoder's TF Hub module\n",
    "- Compute representation\n",
    "- Reduce logging output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/1'.\n",
      "INFO:tensorflow:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/1'.\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_0:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_0\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_1:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_1\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_10:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_10\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_11:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_11\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_12:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_12\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_13:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_13\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_14:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_14\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_15:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_15\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_16:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_16\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_2:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_2\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_3:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_3\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_4:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_4\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_5:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_5\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_6:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_6\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_7:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_7\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_8:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_8\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_9:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_9\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_0/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_1/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_1/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_2/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_2/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/projection:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/projection\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/bias\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/weights\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/bias\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/weights\n",
      "INFO:tensorflow:Initialize variable module/global_step:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with global_step\n",
      "Message: Elephant\n",
      "Embedding size: 512\n",
      "Embedding: [-0.016987277194857597, -0.008949832059442997, -0.007062744814902544, ...]\n",
      "\n",
      "Message: I am a sentence for which I would like to get its embedding.\n",
      "Embedding size: 512\n",
      "Embedding: [0.035313352942466736, -0.025384247303009033, -0.007880019024014473, ...]\n",
      "\n",
      "Message: Universal Sentence Encoder embeddings also support short paragraphs. There is no hard limit on how long the paragraph is. Roughly, the longer the more 'diluted' the embedding will be.\n",
      "Embedding size: 512\n",
      "Embedding: [0.01879095286130905, 0.04536514729261398, -0.020010896027088165, ...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/1\")\n",
    "\n",
    "# Compute a representation for each message, showing various lengths supported.\n",
    "word = \"Elephant\"\n",
    "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
    "paragraph = (\n",
    "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
    "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
    "    \"the more 'diluted' the embedding will be.\")\n",
    "messages = [word, sentence, paragraph]\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "  message_embeddings = session.run(embed(messages))\n",
    "\n",
    "  for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "    print(\"Message: {}\".format(messages[i]))\n",
    "    print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "    message_embedding_snippet = \", \".join(\n",
    "        (str(x) for x in message_embedding[:3]))\n",
    "    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Semantic Textual Similarity Task Example\n",
    "\n",
    "The embeddings produced by the USE are approximately normalized. The semantic similarity of two sentences can be triviially computed as the inner product of the encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_similarity(labels, features, rotation):\n",
    "    corr = np.inner(features, features)\n",
    "    sns.set(font_scale=1.2)\n",
    "    g = sns.heatmap(corr, xticklabels=labels, yticklabels=labels, vmin=0, vmax=1, cmap=\"YlOrRd\")\n",
    "    g.set_xticklabels(labels, rotation=rotation)\n",
    "    g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "\n",
    "def run_and_plot(session_, input_tensor_, messages_, encoding_tensor):\n",
    "  message_embeddings_ = session_.run(\n",
    "      encoding_tensor, feed_dict={input_tensor_: messages})\n",
    "  plot_similarity(messages_, message_embeddings_, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Visualization\n",
    "\n",
    "Here we show the similarity in a heat map. Finla graph is a 9x9 matrix where each entry `[i, j]` is colored based on the inner product of gthe encodings for sentence `i` and `j`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    # Smartphones\n",
    "    \"I like my phone\",\n",
    "    \"My phone is not good.\",\n",
    "    \"Your cellphone looks great.\",\n",
    "\n",
    "    # Weather\n",
    "    \"Will it snow tomorrow?\",\n",
    "    \"Recently a lot of hurricanes have hit the US\",\n",
    "    \"Global warming is real\",\n",
    "\n",
    "    # Food and health\n",
    "    \"An apple a day, keeps the doctors away\",\n",
    "    \"Eating strawberries is healthy\",\n",
    "    \"Is paleo better than keto?\",\n",
    "\n",
    "    # Asking about age\n",
    "    \"How old are you?\",\n",
    "    \"what is your age?\",\n",
    "]\n",
    "\n",
    "similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "similarity_message_encodings = embed(similarity_input_placeholder)\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    run_and_plot(session, similarity_input_placeholder, messages,\n",
    "                 similarity_message_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing the evaluation : STS (Semantic Textual Similarity) Benchmark \n",
    "\n",
    "The [STS Benchmark](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) provides an intristic evaluation of the degree to which similarity scores computed using sentence embeddings align with human judgements. The benchmark requires systems to return similarity scores for a diverse selection of sentence pairs. [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) is then used to evaluate the quality of the machine similarity scores against human judgements.\n",
    "\n",
    "#### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\n",
      "417792/409630 [==============================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "\n",
    "def load_sts_dataset(filename):\n",
    "  # Loads a subset of the STS dataset into a DataFrame. In particular both\n",
    "  # sentences and their human rated similarity score.\n",
    "    sent_pairs = []\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            ts = line.strip().split(\"\\t\")\n",
    "            # (sent_1, sent_2, similarity_score)\n",
    "            sent_pairs.append((ts[5], ts[6], float(ts[4])))\n",
    "            return pandas.DataFrame(sent_pairs, columns=[\"sent_1\", \"sent_2\", \"sim\"])\n",
    "\n",
    "\n",
    "def download_and_load_sts_data():\n",
    "    sts_dataset = tf.keras.utils.get_file(\n",
    "        fname=\"Stsbenchmark.tar.gz\",\n",
    "        origin=\"http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\",\n",
    "        extract=True)\n",
    "    \n",
    "    sts_dev = load_sts_dataset(\n",
    "      os.path.join(os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-dev.csv\"))\n",
    "    sts_test = load_sts_dataset(\n",
    "      os.path.join(\n",
    "          os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-test.csv\"))\n",
    "\n",
    "    return sts_dev, sts_test\n",
    "\n",
    "\n",
    "sts_dev, sts_test = download_and_load_sts_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
