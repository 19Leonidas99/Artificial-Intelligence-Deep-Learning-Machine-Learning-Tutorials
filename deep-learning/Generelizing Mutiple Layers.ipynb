{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization of multiple layers\n",
    "\n",
    "We will cover\n",
    "\n",
    "- Generalization of multiple layers\n",
    "- Minibatches with SGD (Stochastic Gradient Descent)\n",
    "\n",
    "This tutorial generalizes the feedforward neural network into any number of layers. The concepts of a linear projection via matrix multiplication and non-linear transformation will be generalized. The usage of the generalization will be illustrated by building a small feedforward network that consists of two hidden layers to classify handwritten digits. This network will be trained by stochastic gradient descent, a popular variant of gradient descent that updates the parameters each step on only a subset of the parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)\n",
    "from sklearn import datasets, model_selection, metrics # data and evaluation utils\n",
    "from matplotlib.colors import colorConverter, ListedColormap # some plotting functions\n",
    "import itertools\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handwritten digits datasets\n",
    "\n",
    "The dataset used in this tutorial is the [digits dataset](http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html) provided by scikit-learn. This dataset consists of 1797 8x8 images of handwritten digits between 0 and 9. Each 8x8 pixel image is provided as a flattened input vector of 64 variables. Example images for each digit are shown below. \n",
    "\n",
    "Note that this dataset is different from the larger and more famous [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset. The smaller dataset from scikit-learn was chosen to minimize training time for this tutorial. Feel free to experiment and adapt this tutorial to classify the MNIST digits.\n",
    "\n",
    "The dataset will be split into:\n",
    "\n",
    "- A training set used to train the model. (inputs: X_train, targets: T_train)\n",
    "- A validation set used to validate the model performance and to stop training if the model starts [overfitting](https://en.wikipedia.org/wiki/Overfitting) on the training data. (inputs: X_validation, targets: T_validation)\n",
    "- A final test set to evaluate the trained model on data is independent of the training and validation data. (inputs: X_test, targets: T_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from scikit-learn.\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Load the targets.\n",
    "# Note that the targets are stored as digits, these need to be \n",
    "#  converted to one-hot-encoding for the output sofmax layer.\n",
    "T = np.zeros((digits.target.shape[0],10))\n",
    "T[np.arange(len(T)), digits.target] += 1\n",
    "\n",
    "# Divide the data into a train and test set.\n",
    "X_train, X_test, T_train, T_test = model_selection.train_test_split(\n",
    "    digits.data, T, test_size=0.4)\n",
    "# Divide the test set into a validation set and final test set.\n",
    "X_validation, X_test, T_validation, T_test = model_selection.train_test_split(\n",
    "    X_test, T_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzEAAABxCAYAAADlJi1xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/CUgVSbiBAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11393d390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot an example of each image.\n",
    "fig = plt.figure(figsize=(10, 1), dpi=100)\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(1,10,i+1)\n",
    "    ax.matshow(digits.images[i], cmap='binary') \n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization of the layers\n",
    "In Vectorization of this tutorial series took on the classical view of neural networks where each layer consists of a linear transformation by a matrix multiplication and vector addition followed by a non-linear function.\n",
    "\n",
    "Here the linear transformation is split from the non-linear function and each is abstracted into its own layer. This has the benefit that the forward and backward step of each layer can easily be calculated separately.\n",
    "\n",
    "This tutorial defines three example layers as Python classes:\n",
    "\n",
    "- A layer to apply the linear transformation (LinearLayer).\n",
    "- A layer to apply the logistic function (LogisticLayer).\n",
    "- A layer to compute the softmax classification probabilities at the output (SoftmaxOutputLayer).\n",
    "\n",
    "Each layer can compute its output in the forward step with get_output, which can then be used as the input for the next layer. The gradient at the input of each layer in the backpropagation step can be computed with get_input_grad. This function computes the gradient with the help of the targets if it's the last layer, or the gradients at its outputs (gradients at input of next layer) if it's an intermediate layer. Each layer has the option to iterate over the parameters (if any) with get_params_iter, and get the gradients of these parameters in the same order with get_params_grad.\n",
    "\n",
    "Notice that the gradient and cost computed by the softmax layer are divided by the number of input samples. This is to make this gradient and cost independent of the number of input samples so that the size of the mini-batches can be changed without affecting other parameters. More on this later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the non-linear functions used\n",
    "def logistic(z): \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_deriv(y):  # Derivative of logistic function\n",
    "    return np.multiply(y, (1 - y))\n",
    "    \n",
    "def softmax(z): \n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the layers used in this model\n",
    "class Layer(object):\n",
    "    \"\"\"Base class for the different layers.\n",
    "    Defines base methods and documentation of methods.\"\"\"\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters (if any).\n",
    "        The iterator has the same order as get_params_grad.\n",
    "        The elements returned by the iterator are editable in-place.\"\"\"\n",
    "        return []\n",
    "    \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"Return a list of gradients over the parameters.\n",
    "        The list has the same order as the get_params_iter iterator.\n",
    "        X is the input.\n",
    "        output_grad is the gradient at the output of this layer.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step linear transformation.\n",
    "        X is the input.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad=None, T=None):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\n",
    "        Y is the pre-computed output of this layer (not needed in this case).\n",
    "        output_grad is the gradient at the output of this layer \n",
    "         (gradient at input of next layer).\n",
    "        Output layer uses targets T to compute the gradient based on the \n",
    "         output error instead of output_grad\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearLayer(Layer):\n",
    "    \"\"\"The linear layer performs a linear transformation to its input.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"Initialize hidden layer parameters.\n",
    "        n_in is the number of input variables.\n",
    "        n_out is the number of output variables.\"\"\"\n",
    "        self.W = np.random.randn(n_in, n_out) * 0.1\n",
    "        self.b = np.zeros(n_out)\n",
    "        \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters.\"\"\"\n",
    "        return itertools.chain(np.nditer(self.W, op_flags=['readwrite']),\n",
    "                               np.nditer(self.b, op_flags=['readwrite']))\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step linear transformation.\"\"\"\n",
    "        return X.dot(self.W) + self.b\n",
    "        \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"Return a list of gradients over the parameters.\"\"\"\n",
    "        JW = X.T.dot(output_grad)\n",
    "        Jb = np.sum(output_grad, axis=0)\n",
    "        return [g for g in itertools.chain(np.nditer(JW), np.nditer(Jb))]\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return output_grad.dot(self.W.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticLayer(Layer):\n",
    "    \"\"\"The logistic layer applies the logistic function to its inputs.\"\"\"\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return logistic(X)\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return np.multiply(logistic_deriv(Y), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxOutputLayer(Layer):\n",
    "    \"\"\"The softmax output layer computes the classification propabilities at the output.\"\"\"\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return softmax(X)\n",
    "    \n",
    "    def get_input_grad(self, Y, T):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return (Y - T) / Y.shape[0]\n",
    "    \n",
    "    def get_cost(self, Y, T):\n",
    "        \"\"\"Return the cost at the output of this output layer.\"\"\"\n",
    "        return - np.multiply(T, np.log(Y)).sum() / Y.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample model\n",
    "\n",
    "The following sections will refer to a layer as a layer defined above, and hidden-layer or output-layer as the classical neural network view of a linear transformation followed by a non-linear function.\n",
    "\n",
    "The sample model used to classify the handwritten digits in this tutorial consists of two hidden-layers with logistic functions and a softmax output-layer. The fist hidden-layer takes a vector of 64 pixel values and transforms them to a vector of 20 values. The second hidden-layer projects the previous 20 values to 20 new values. The output-layer outputs probabilities for the 10 possible classes. This architecture is illustrated in the following figure (biases are not shown to keep figure clean).\n",
    "\n",
    "<Image src='images/SampleModel.png'>\n",
    "\n",
    "The full network is represented as a sequential list where each next layer is added on top of the previous layer by putting it in the next position in the list. The first layer is at position 0 in this list, the last layer is at the last index of this list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a sample model to be trained on the data\n",
    "hidden_neurons_1 = 20  # Number of neurons in the first hidden-layer\n",
    "hidden_neurons_2 = 20  # Number of neurons in the second hidden-layer\n",
    "# Create the model\n",
    "layers = [] # Define a list of layers\n",
    "# Add first hidden layer\n",
    "layers.append(LinearLayer(X_train.shape[1], hidden_neurons_1))\n",
    "layers.append(LogisticLayer())\n",
    "# Add second hidden layer\n",
    "layers.append(LinearLayer(hidden_neurons_1, hidden_neurons_2))\n",
    "layers.append(LogisticLayer())\n",
    "# Add output layer\n",
    "layers.append(LinearLayer(hidden_neurons_2, T_train.shape[1]))\n",
    "layers.append(SoftmaxOutputLayer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "The details of how backpropagation works in the forward and backward step of vectorized model are explained in part 4 of this tutorial. This section will only illustrate how to perform backpropagation over any number of layers with the generalized model described here.\n",
    "\n",
    "#### Forward step\n",
    "\n",
    "The forward steps are computed by the forward_step method defined below. This method iteratively computes the outputs of each layer and feeds it as input to the next layer until the last layer. Each layer's output is computed by calling the get_output method. These output activations are stored in the activations list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the forward propagation step as a method.\n",
    "def forward_step(input_samples, layers):\n",
    "    \"\"\"\n",
    "    Compute and return the forward activation of each layer in layers.\n",
    "    Input:\n",
    "        input_samples: A matrix of input samples (each row is an input vector)\n",
    "        layers: A list of Layers\n",
    "    Output:\n",
    "        A list of activations where the activation at each index i+1 corresponds to\n",
    "        the activation of layer i in layers. activations[0] contains the input samples.  \n",
    "    \"\"\"\n",
    "    activations = [input_samples] # List of layer activations\n",
    "    # Compute the forward activations for each layer starting from the first\n",
    "    X = input_samples\n",
    "    for layer in layers:\n",
    "        Y = layer.get_output(X)  # Get the output of the current layer\n",
    "        activations.append(Y)  # Store the output for future processing\n",
    "        X = activations[-1]  # Set the current input as the activations of the previous layer\n",
    "    return activations  # Return the activations of each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward step\n",
    "\n",
    "The gradients computed in the backward step are computed by the backward_step method defined below. The backward step goes over all the layers in the reversed order. It first gets the initial gradients from the output layer and uses these gradients to compute the gradients of the layers below by iteratively calling the get_input_grad method. During each step, it computes the gradients of the cost with respect to the parameters by calling the get_params_grad method and returns them in a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the backward propagation step as a method\n",
    "def backward_step(activations, targets, layers):\n",
    "    \"\"\"\n",
    "    Perform the backpropagation step over all the layers and return the parameter gradients.\n",
    "    Input:\n",
    "        activations: A list of forward step activations where the activation at \n",
    "            each index i+1 corresponds to the activation of layer i in layers. \n",
    "            activations[0] contains the input samples. \n",
    "        targets: The output targets of the output layer.\n",
    "        layers: A list of Layers corresponding that generated the outputs in activations.\n",
    "    Output:\n",
    "        A list of parameter gradients where the gradients at each index corresponds to\n",
    "        the parameters gradients of the layer at the same index in layers. \n",
    "    \"\"\"\n",
    "    param_grads = collections.deque()  # List of parameter gradients for each layer\n",
    "    output_grad = None  # The error gradient at the output of the current layer\n",
    "    # Propagate the error backwards through all the layers.\n",
    "    #  Use reversed to iterate backwards over the list of layers.\n",
    "    for layer in reversed(layers):   \n",
    "        Y = activations.pop()  # Get the activations of the last layer on the stack\n",
    "        # Compute the error at the output layer.\n",
    "        # The output layer error is calculated different then hidden layer error.\n",
    "        if output_grad is None:\n",
    "            input_grad = layer.get_input_grad(Y, targets)\n",
    "        else:  # output_grad is not None (layer is not output layer)\n",
    "            input_grad = layer.get_input_grad(Y, output_grad)\n",
    "        # Get the input of this layer (activations of the previous layer)\n",
    "        X = activations[-1]\n",
    "        # Compute the layer parameter gradients used to update the parameters\n",
    "        grads = layer.get_params_grad(X, output_grad)\n",
    "        param_grads.appendleft(grads)\n",
    "        # Compute gradient at output of previous layer (input of current layer):\n",
    "        output_grad = input_grad\n",
    "    return list(param_grads)  # Return the parameter gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Checking\n",
    "As in part Vectorization, the gradient computed by backpropagation is compared with the numerical gradient to assert that there are no bugs in the code to compute the gradients.\n",
    "\n",
    "The code below gets the parameters of each layer by the help of the get_params_iter method that returns an iterator over all the parameters in the layer. The order of parameters returned corresponds to the order of parameter gradients returned by get_params_grad during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient checking\n",
    "nb_samples_gradientcheck = 10 # Test the gradients on a subset of the data\n",
    "X_temp = X_train[0:nb_samples_gradientcheck,:]\n",
    "T_temp = T_train[0:nb_samples_gradientcheck,:]\n",
    "# Get the parameter gradients with backpropagation\n",
    "activations = forward_step(X_temp, layers)\n",
    "param_grads = backward_step(activations, T_temp, layers)\n",
    "\n",
    "# Set the small change to compute the numerical gradient\n",
    "eps = 0.0001\n",
    "# Compute the numerical gradients of the parameters in all layers.\n",
    "for idx in range(len(layers)):\n",
    "    layer = layers[idx]\n",
    "    layer_backprop_grads = param_grads[idx]\n",
    "    # Compute the numerical gradient for each parameter in the layer\n",
    "    for p_idx, param in enumerate(layer.get_params_iter()):\n",
    "        grad_backprop = layer_backprop_grads[p_idx]\n",
    "        # + eps\n",
    "        param += eps\n",
    "        plus_cost = layers[-1].get_cost(forward_step(X_temp, layers)[-1], T_temp)\n",
    "        # - eps\n",
    "        param -= 2 * eps\n",
    "        min_cost = layers[-1].get_cost(forward_step(X_temp, layers)[-1], T_temp)\n",
    "        # reset param value\n",
    "        param += eps\n",
    "        # calculate numerical gradient\n",
    "        grad_num = (plus_cost - min_cost)/(2*eps)\n",
    "        # Raise error if the numerical grade is not close to the backprop gradient\n",
    "        if not np.isclose(grad_num, grad_backprop):\n",
    "            raise ValueError('Numerical gradient of {:.6f} is not close to the backpropagation gradient of {:.6f}!'.format(float(grad_num), float(grad_backprop)))\n",
    "print('No gradient errors found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Stochastic gradient descent backpropagation\n",
    "This tutorial uses a variant of gradient descent called Stochastic gradient descent (SGD) to optimize the cost function. \n",
    "SGD follows the negative gradient of the cost function on subsets of the total training set. This has a few benefits: One of them is that the training time on large datasets can be reduced because the matrix of samples is much smaller during each sub-iteration and the gradients can be computed faster and with less memory. Another benefit is that computing the cost function on subsets results in noise, i.e. each subset will give a different cost depending on the samples. This will result in noisy (stochastic) gradient updates which might be able to push the gradient descent out of local minima. These, and other, benefits contribute to the popularity of SGD on training large scale machine learning methods such as neural networks.\n",
    "\n",
    "The cost function needs to be independent of the number of input samples because the size of the subsets used during SGD can vary. This is why the mean squared error (MSE) cost function is used instead of just the squared error. Using the mean instead of the sum is reflected in the gradient and cost computed by the softmax layer being divided by the number of input samples.\n",
    "\n",
    "#### Minibatches\n",
    "\n",
    "The subsets of the training set are often called mini-batches. The following code will divide the training set into mini-batches of around 25 samples per batch. The inputs and targets are combined together in a list of (input, target) tuples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the minibatches\n",
    "batch_size = 25  # Approximately 25 samples per batch\n",
    "nb_of_batches = X_train.shape[0] / batch_size  # Number of batches\n",
    "# Create batches (X,Y) from the training set\n",
    "XT_batches = zip(\n",
    "    np.array_split(X_train, nb_of_batches, axis=0),  # X samples\n",
    "    np.array_split(T_train, nb_of_batches, axis=0))  # Y targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD updates\n",
    "\n",
    "The parameters $\\mathbf{\\theta}$ of the network are updated by the update_params method that iterates over each parameter of each layer and applies the simple [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) rule on each mini-batch: $\\mathbf{\\theta}(k+1) = \\mathbf{\\theta}(k) - \\Delta \\mathbf{\\theta}(k+1)$. $\\Delta \\mathbf{\\theta}$ is defined as: $\\Delta \\mathbf{\\theta} = \\mu \\frac{\\partial \\xi}{\\partial \\mathbf{\\theta}}$ with $\\mu$ the learning rate.\n",
    "\n",
    "The update steps will be performed for a number of iterations (nb_of_iterations) over the full training set, where each full iterations consists of multiple updates over the mini-batches. After each full iteration, the resulting network will be tested on the validation set. The training will stop if the cost on the validation set doesn't increase after three full iterations to prevent overfitting or after maximum 300 iterations. All costs will be stored in between for future analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a method to update the parameters\n",
    "def update_params(layers, param_grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Function to update the parameters of the given layers with the given gradients\n",
    "    by gradient descent with the given learning rate.\n",
    "    \"\"\"\n",
    "    for layer, layer_backprop_grads in zip(layers, param_grads):\n",
    "        for param, grad in itertools.zip(layer.get_params_iter(), layer_backprop_grads):\n",
    "            # The parameter returned by the iterator point to the memory space of\n",
    "            #  the original layer and can thus be modified inplace.\n",
    "            param -= learning_rate * grad  # Update each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform backpropagation\n",
    "# initalize some lists to store the cost for future analysis        \n",
    "minibatch_costs = []\n",
    "training_costs = []\n",
    "validation_costs = []\n",
    "\n",
    "max_nb_of_iterations = 300  # Train for a maximum of 300 iterations\n",
    "learning_rate = 0.1  # Gradient descent learning rate\n",
    "\n",
    "# Train for the maximum number of iterations\n",
    "for iteration in range(max_nb_of_iterations):\n",
    "    for X, T in XT_batches:  # For each minibatch sub-iteration\n",
    "        activations = forward_step(X, layers)  # Get the activations\n",
    "        minibatch_cost = layers[-1].get_cost(activations[-1], T)  # Get cost\n",
    "        minibatch_costs.append(minibatch_cost)\n",
    "        param_grads = backward_step(activations, T, layers)  # Get the gradients\n",
    "        update_params(layers, param_grads, learning_rate)  # Update the parameters\n",
    "    # Get full training cost for future analysis (plots)\n",
    "    activations = forward_step(X_train, layers)\n",
    "    train_cost = layers[-1].get_cost(activations[-1], T_train)\n",
    "    training_costs.append(train_cost)\n",
    "    # Get full validation cost\n",
    "    activations = forward_step(X_validation, layers)\n",
    "    validation_cost = layers[-1].get_cost(activations[-1], T_validation)\n",
    "    validation_costs.append(validation_cost)\n",
    "    if len(validation_costs) > 3:\n",
    "        # Stop training if the cost on the validation set doesn't decrease\n",
    "        #  for 3 iterations\n",
    "        if validation_costs[-1] >= validation_costs[-2] >= validation_costs[-3]:\n",
    "            break\n",
    "    \n",
    "nb_of_iterations = iteration + 1  # The number of iterations that have been executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The costs stored during training can be plotted to visualize the performance during training. The resulting plot is shown in the next figure. The cost on the training samples and validation samples goes down very quickly and flattens out after about 40 iterations on the full training set. Notice that the cost on the training set is lower than the cost on the validation set, this is because the network is optimized on the training set and is slightly overfitting. The training stops after around 90 iterations because the validation cost stops decreasing.\n",
    "Also, notice that the cost of the mini-batches fluctuates around the cost of the full training set. This is the stochastic effect of mini-batches in SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the minibatch, full training set, and validation costs\n",
    "minibatch_x_inds = np.linspace(0, nb_of_iterations, num=nb_of_iterations*nb_of_batches)\n",
    "iteration_x_inds = np.linspace(1, nb_of_iterations, num=nb_of_iterations)\n",
    "# Plot the cost over the iterations\n",
    "plt.plot(minibatch_x_inds, minibatch_costs, 'k-', linewidth=0.5, label='cost minibatches')\n",
    "plt.plot(iteration_x_inds, training_costs, 'r-', linewidth=2, label='cost full training set')\n",
    "plt.plot(iteration_x_inds, validation_costs, 'b-', linewidth=3, label='cost validation set')\n",
    "# Add labels to the plot\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('$\\\\xi$', fontsize=15)\n",
    "plt.title('Decrease of cost over backprop iteration')\n",
    "plt.legend()\n",
    "x1,x2,y1,y2 = plt.axis()\n",
    "plt.axis((0,nb_of_iterations,0,2.5))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
